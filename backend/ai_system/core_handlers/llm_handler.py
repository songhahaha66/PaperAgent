"""
LLMé€šä¿¡å¤„ç†å™¨
é‡æ„æ”¯æŒå¤šç§AIæä¾›å•†çš„LLMé€šä¿¡å¤„ç†å™¨ï¼ŒåŸºäº LangChain Agent
"""

import logging
import json
from typing import List, Dict, Any, Optional, AsyncGenerator
from langchain_core.language_models import BaseLanguageModel
from langchain.agents import create_agent
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# ä½¿ç”¨å­—ç¬¦ä¸²ç±»å‹æ³¨è§£é¿å…å¾ªç¯å¯¼å…¥ï¼ˆç±»å‹ä»ç”¨å­—ç¬¦ä¸²ï¼‰
# è¿è¡ŒæœŸéœ€è¦çš„é…ç½®æ¨¡å—å®‰å…¨åœ°åœ¨æ¨¡å—çº§å¯¼å…¥
from ..config.async_config import AsyncConfig
from .llm_providers import LLMProviderFactory, create_llm_from_model_config

logger = logging.getLogger(__name__)


class LLMHandler:
    """
    æ”¯æŒå¤šAIæä¾›å•†çš„LLMå¤„ç†å™¨ï¼ŒåŸºäº LangChain Agentï¼Œæ”¯æŒæŒ‰æ­¥éª¤æµå¼ä¼ è¾“ä»£ç†è¿›åº¦
    """

    def __init__(self,
                 model: Optional[str] = None,
                 stream_manager: Optional['StreamOutputManager'] = None,
                 provider: Optional[str] = None,
                 api_key: Optional[str] = None,
                 base_url: Optional[str] = None,
                 model_config: Optional[Any] = None,
                 **llm_kwargs):
        """
        åˆå§‹åŒ–LLMå¤„ç†å™¨

        Args:
            model: æ¨¡å‹IDï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›äº†model_configåˆ™ä»ä¸­è·å–ï¼‰
            stream_manager: æµå¼è¾“å‡ºç®¡ç†å™¨
            provider: AIæä¾›å•†åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›äº†model_configåˆ™ä»ä¸­è·å–ï¼‰
            api_key: APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›äº†model_configåˆ™ä»ä¸­è·å–ï¼‰
            base_url: APIåŸºç¡€URLï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›äº†model_configåˆ™ä»ä¸­è·å–ï¼‰
            model_config: æ•°æ®åº“ModelConfigå¯¹è±¡
            **llm_kwargs: ä¼ é€’ç»™LLMå®ä¾‹çš„é¢å¤–å‚æ•°
        """
        self.stream_manager = stream_manager
        self.llm_kwargs = {
            'temperature': llm_kwargs.get('temperature', 0.7),
            'max_tokens': llm_kwargs.get('max_tokens', 4000),
            'streaming': llm_kwargs.get('streaming', True)
        }

        # æ„å»ºé…ç½®å­—å…¸
        if model_config:
            # ä»æ•°æ®åº“é…ç½®å¯¹è±¡åˆ›å»ºLLMå®ä¾‹
            self.llm = create_llm_from_model_config(model_config, **self.llm_kwargs)
            self.provider = getattr(model_config, 'provider', 'openai')
            self.model = model_config.model_id
            logger.info(f"LLMHandleråˆå§‹åŒ–å®Œæˆï¼Œæä¾›å•†: {self.provider}, æ¨¡å‹: {self.model}")
        else:
            # ä»ç›´æ¥å‚æ•°åˆ›å»ºLLMå®ä¾‹
            config = {
                'provider': provider or 'openai',
                'model_id': model,
                'api_key': api_key,
                'base_url': base_url,
                'is_active': True
            }
            self.llm = LLMProviderFactory.create_llm_instance(config, **self.llm_kwargs)
            self.provider = provider or 'openai'
            self.model = model
            logger.info(f"LLMHandleråˆå§‹åŒ–å®Œæˆï¼Œæä¾›å•†: {self.provider}, æ¨¡å‹: {self.model}")

        # åˆ›å»ºè®ºæ–‡å†™ä½œä¸“ç”¨çš„ prompt
        self.system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®ºæ–‡å†™ä½œåŠ©æ‰‹ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·ç”Ÿæˆã€ä¿®æ”¹å’Œå®Œå–„å­¦æœ¯è®ºæ–‡ã€‚è¯·ä»”ç»†åˆ†æç”¨æˆ·éœ€æ±‚ï¼Œå¹¶è°ƒç”¨ç›¸åº”çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚"
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", self.system_prompt),
            MessagesPlaceholder(variable_name="chat_history", optional=True),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])

        logger.info(f"å¤šæä¾›å•†LLMHandleråˆå§‹åŒ–å®Œæˆï¼Œæä¾›å•†: {self.provider}, æ¨¡å‹: {self.model}")

    def _convert_messages_to_input(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸º Agent è¾“å…¥æ ¼å¼"""
        # æ–°ç‰ˆ LangChain agent æœŸæœ› messages æ ¼å¼
        return {
            "messages": messages
        }

    def _convert_to_langchain_messages(self, messages: List[Dict[str, Any]]) -> List:
        """å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸º LangChain æ¶ˆæ¯æ ¼å¼"""
        langchain_messages = []

        for message in messages:
            role = message.get("role", "")
            content = message.get("content", "")

            if role == "system":
                langchain_messages.append(SystemMessage(content=content))
            elif role == "user":
                langchain_messages.append(HumanMessage(content=content))
            elif role == "assistant":
                langchain_messages.append(AIMessage(content=content))
            else:
                # é»˜è®¤ä½œä¸ºç”¨æˆ·æ¶ˆæ¯å¤„ç†
                langchain_messages.append(HumanMessage(content=content))

        return langchain_messages

    async def _handle_agent_step(self, step: str, data: Dict[str, Any]) -> str:
        """å¤„ç†ä»£ç†æ­¥éª¤å¹¶è¿”å›å¯æ˜¾ç¤ºçš„å†…å®¹"""
        if step == "agent":
            # LLM èŠ‚ç‚¹ï¼Œå¤„ç† AI å“åº”
            if "messages" in data and data["messages"]:
                message = data["messages"][-1]
                if hasattr(message, 'content') and message.content:
                    return message.content
                elif hasattr(message, 'content_blocks'):
                    # å¤„ç†å†…å®¹å—
                    content = ""
                    for block in message.content_blocks:
                        if hasattr(block, 'text'):
                            content += block.text
                    return content

        elif step == "tools":
            # å·¥å…·èŠ‚ç‚¹ï¼Œå¤„ç†å·¥å…·æ‰§è¡Œç»“æœ
            if "messages" in data and data["messages"]:
                message = data["messages"][-1]
                if hasattr(message, 'content'):
                    return f"\nğŸ”§ å·¥å…·æ‰§è¡Œç»“æœ: {message.content}\n"

        return ""

    async def process_stream(self, messages: List[Dict[str, Any]], tools: Optional[List[Any]] = None):
        """
        åŸºäº LangChain Agent çš„æµå¼å¤„ç†ï¼ŒæŒ‰æ­¥éª¤ä¼ è¾“ä»£ç†è¿›åº¦
        """
        logger.info(f"å¼€å§‹è°ƒç”¨ LangChain Agentï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}")

        if tools:
            logger.info(f"ä½¿ç”¨å·¥å…·æ•°é‡: {len(tools)}")

        try:
            # å¦‚æœæœ‰å·¥å…·ï¼Œåˆ›å»º agent
            if tools:
                # è½¬æ¢å·¥å…·ä¸º LangChain æ ¼å¼
                agent = create_agent(self.llm, tools=tools, system_prompt=self.system_prompt)

                # è½¬æ¢æ¶ˆæ¯æ ¼å¼
                input_data = self._convert_messages_to_input(messages)

                # ç®€åŒ–å¤„ç†ï¼šç›´æ¥è°ƒç”¨ agent
                try:
                    result = await agent.ainvoke(input_data)

                    # æ–°ç‰ˆ LangChain agent è¿”å› messages åˆ—è¡¨
                    messages = result.get("messages", [])
                    content = ""

                    # æ‰¾åˆ°æœ€åä¸€æ¡ AI æ¶ˆæ¯
                    for message in reversed(messages):
                        if isinstance(message, dict) and message.get("role") == "assistant":
                            content = message.get("content", "")
                            break
                        elif hasattr(message, 'content'):
                            content = message.content or ""
                            break

                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ° AI æ¶ˆæ¯ï¼Œä½¿ç”¨ messages çš„æœ€åä¸€ä¸ªå…ƒç´ 
                    if not content and messages:
                        last_message = messages[-1]
                        if isinstance(last_message, dict):
                            content = last_message.get("content", "")
                        elif hasattr(last_message, 'content'):
                            content = last_message.content or ""

                    # å‘é€åˆ°æµå¼ç®¡ç†å™¨
                    if self.stream_manager:
                        await self.stream_manager.print_stream(content)
                        await self.stream_manager.finalize_message()
                    else:
                        print(content)

                    assistant_message = {"role": "assistant", "content": content}
                    return assistant_message, []

                except Exception as e:
                    logger.error(f"Agent è°ƒç”¨å¤±è´¥: {e}")
                    # é™çº§ä¸ºç›´æ¥ LLM è°ƒç”¨
                    langchain_messages = self._convert_to_langchain_messages(messages)
                    response = await self.llm.ainvoke(langchain_messages)

                    content = response.content or ""

                    if self.stream_manager:
                        await self.stream_manager.print_stream(content)
                        await self.stream_manager.finalize_message()
                    else:
                        print(content)

                    return {"role": "assistant", "content": content}, []

            else:
                # æ²¡æœ‰å·¥å…·çš„æƒ…å†µï¼Œç›´æ¥è°ƒç”¨ LLM
                langchain_messages = self._convert_to_langchain_messages(messages)
                response = await self.llm.ainvoke(langchain_messages)

                content = response.content or ""

                if self.stream_manager:
                    await self.stream_manager.print_stream(content)
                    await self.stream_manager.finalize_message()
                else:
                    print(content)

                logger.info(f"LangChain LLM è°ƒç”¨å®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(content)}")

                return {"role": "assistant", "content": content}, []

        except Exception as e:
            logger.error(f"LangChain è°ƒç”¨å¤±è´¥: {e}", exc_info=True)
            error_message = f"LLM è°ƒç”¨å¤±è´¥: {str(e)}"
            if self.stream_manager:
                await self.stream_manager.print_content(error_message)

            return {"role": "assistant", "content": error_message}, []

    async def process_sync(self, messages: List[Dict[str, Any]], tools: Optional[List[Any]] = None):
        """
        åŒæ­¥è°ƒç”¨ LangChain LLMï¼ˆéæµå¼ï¼‰ï¼Œç”¨äºä¸éœ€è¦æµå¼å¤„ç†çš„åœºæ™¯
        """
        logger.info(f"å¼€å§‹åŒæ­¥è°ƒç”¨ LangChain LLMï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}")

        try:
            # å¦‚æœæœ‰å·¥å…·ï¼Œåˆ›å»º agent
            if tools:
                agent = create_agent(self.llm, tools=tools, system_prompt=self.system_prompt)

                # è½¬æ¢æ¶ˆæ¯æ ¼å¼
                input_data = self._convert_messages_to_input(messages)

                # åŒæ­¥è°ƒç”¨
                result = await agent.ainvoke(input_data)

                # æ–°ç‰ˆ LangChain agent è¿”å› messages åˆ—è¡¨
                messages = result.get("messages", [])
                content = ""

                # æ‰¾åˆ°æœ€åä¸€æ¡ AI æ¶ˆæ¯
                for message in reversed(messages):
                    if isinstance(message, dict) and message.get("role") == "assistant":
                        content = message.get("content", "")
                        break
                    elif hasattr(message, 'content'):
                        content = message.content or ""
                        break

                # å¦‚æœæ²¡æœ‰æ‰¾åˆ° AI æ¶ˆæ¯ï¼Œä½¿ç”¨ messages çš„æœ€åä¸€ä¸ªå…ƒç´ 
                if not content and messages:
                    last_message = messages[-1]
                    if isinstance(last_message, dict):
                        content = last_message.get("content", "")
                    elif hasattr(last_message, 'content'):
                        content = last_message.content or ""

                logger.info(f"åŒæ­¥ LangChain Agent è°ƒç”¨å®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(content)}")

                return {"role": "assistant", "content": content}, []

            else:
                # æ²¡æœ‰å·¥å…·çš„æƒ…å†µï¼Œç›´æ¥è°ƒç”¨ LLM
                langchain_messages = self._convert_to_langchain_messages(messages)
                response = await self.llm.ainvoke(langchain_messages)

                content = response.content or ""

                logger.info(f"åŒæ­¥ LangChain LLM è°ƒç”¨å®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(content)}")

                return {"role": "assistant", "content": content}, []

        except Exception as e:
            logger.error(f"åŒæ­¥ LangChain LLM è°ƒç”¨å¤±è´¥: {e}", exc_info=True)
            error_message = f"LLM è°ƒç”¨å¤±è´¥: {str(e)}"
            return {"role": "assistant", "content": error_message}, []

    def set_model(self, model: str, provider: Optional[str] = None, **kwargs):
        """
        è®¾ç½®LLMæ¨¡å‹å’Œæä¾›å•†

        Args:
            model: æ¨¡å‹ID
            provider: AIæä¾›å•†åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å½“å‰æä¾›å•†ï¼‰
            **kwargs: å…¶ä»–LLMå‚æ•°
        """
        self.model = model
        if provider:
            self.provider = provider

        # æ„å»ºé…ç½®å¹¶åˆ›å»ºæ–°çš„LLMå®ä¾‹
        config = {
            'provider': self.provider or 'openai',
            'model_id': model,
            'api_key': kwargs.get('api_key', ''),
            'base_url': kwargs.get('base_url', ''),
            'is_active': True
        }

        # æ›´æ–°LLMå‚æ•°
        llm_params = {**self.llm_kwargs, **kwargs}
        self.llm = LLMProviderFactory.create_llm_instance(config, **llm_params)
        logger.info(f"LLMå·²æ›´æ–°ï¼Œæä¾›å•†: {self.provider}, æ¨¡å‹: {model}")

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰æ¨¡å‹ä¿¡æ¯"""
        return {
            "provider": self.provider,
            "model": self.model,
            "stream_manager_configured": self.stream_manager is not None,
            "langchain_based": True,
            "supported_providers": LLMProviderFactory.get_supported_providers()
        }

    async def close(self):
        """æ¸…ç†èµ„æº"""
        logger.info("LangChain LLMHandler èµ„æºæ¸…ç†å®Œæˆ")
